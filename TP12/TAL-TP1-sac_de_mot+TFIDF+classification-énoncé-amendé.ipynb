{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92321de7",
   "metadata": {},
   "source": [
    "# TAL - TD 1\n",
    "\n",
    "On va recycler le code vu en CM (démo.ipynb), faire 2 encodages à la main, et utiliser des algos de classification:\n",
    "\n",
    "# Plan du TP \n",
    "\n",
    "(détail et guidage plus bas, évidemment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67951c36-7f47-4a11-a5ed-45e61c985897",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Chargement des données \n",
    "Cette partie est offerte par la maison ! On vous donne le code, qui est un copié-collé du code montré en cours magistral lors de la séance précédente: voir le fichier `TAL-1-démo de cours.ipynb` pour avoir plus d'explications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ddb83",
   "metadata": {},
   "source": [
    "## 1. Encodage\n",
    "On va travailler l'encodage en le faisant à la main, pour tout de même bien comprendre. On fait ça sur le dataset des vidéos youtube sous-titrées:\n",
    "\n",
    "    - 1.1 coder la méthode du sac de mots, avec CountVectorizer en backup,\n",
    "    - 1.2 coder TF-IDF, avec TfidfVectorizer en backup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d134a19",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "Pour limiter l'overfitting en classification, on va prendre un plus gros dataset (20newsgroup, ou bien les 3 journaux). On utilise pour la suite les encodeurs de sklearn, qui sont sans erreurs et plus rapides que les votres)\n",
    "\n",
    "    - 2.1 utiliser un modèle Bayésien Naïf (sans PCA) comme classifieur  (pour le sac de mot, vs TF-IDF)\n",
    "    - 2.2 [optionnel] (si le temps permet) utiliser une PCA(D' variable)+k-NN(n variable) comme classifieur (pour le sac de mot, vs TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02746ed7",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Chargement des données \n",
    "Cette partie est offerte par la maison ! On vous donne le code, qui est un copié-collé du code montré en cours magistral lors de la séance précédente: voir le fichier `TAL-1-démo de cours.ipynb` pour avoir plus d'explications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  ## classique pour les plots\n",
    "import matplotlib.cm as cm\n",
    "# import seaborn as sns            ## autre librairie pour faire de jolis plots\n",
    "import sklearn.decomposition     ## Pour la PCA\n",
    "\n",
    "## chargement des données:\n",
    "import pandas as pd\n",
    "import json  # pour importer des fichiers json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # retour au défaut : 'default'\n",
    "from sklearn.datasets import fetch_20newsgroups ## partie 2.1 (plus gros jeu de données)\n",
    "\n",
    "## tokenization manuelle:\n",
    "import re ## regular expressions\n",
    "from collections import Counter      # un package qui permet de faire un compteur d'occcurences (de types par exemple) en qq lignes.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer ## partie 1.1, pour tricher/vérifier votre code.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer ## partie 1.2, pour tricher/vérifier votre code.\n",
    "\n",
    "import sklearn # partie 2\n",
    "from sklearn.naive_bayes import MultinomialNB ## partie 2.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80473892",
   "metadata": {},
   "source": [
    "### Chargement et Tokenization (offerts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un fichier json contenant les sous titres de plein de vidéos youtube\n",
    "videos = json.loads(open('french_yt_videos.json').read())\n",
    "\n",
    "themes_d_interet = ['Science & Technology', 'News & Politics', 'Comedy']\n",
    "texts = []\n",
    "ys = []\n",
    "for i, entree in enumerate(videos):\n",
    "    if videos[i]['category'] in themes_d_interet:\n",
    "        texts.append(entree['text'].lower())\n",
    "        ys   .append(entree['category'])\n",
    "        \n",
    "## tokenization (on vous la donne (rappelle))\n",
    "regexp = r\"\\b\\w+\\b\"\n",
    "retoken = re.compile(regexp)\n",
    "# tokenisons d'abord (texte par texte)\n",
    "tokens = [retoken.findall(text) for text in texts]\n",
    "\n",
    "# del texts ## ca éviterait d'oublier de prendre la version tokenizée.. mais sklearn en a besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190fdae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## tokens est une liste de listes: chaque élément de tokens est un texte (document) du corpus, \n",
    "## et cet élément (un document) est lui meme une liste de tokens\n",
    "## on peut regarder par exemple les 20 premiers tokens du premier texte:\n",
    "tokens[0] [:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2578a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## il nous reste combien de points de données (=combien de documents dans notre corpus) ?\n",
    "N = len(tokens)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10320abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on passe les labels sous forme numérique, c'est parfois pratique:\n",
    "## (ça nous servira seulement au moment de classifier)\n",
    "encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_numerique = encoder.fit_transform(ys)\n",
    "y_numerique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16b7a7",
   "metadata": {},
   "source": [
    "## 1. Encodage\n",
    "On va travailler l'encodage en le faisant à la main, pour tout de même bien comprendre. On fait ça sur le dataset des vidéos youtube sous-titrées:\n",
    "\n",
    "    - 1.1 coder la méthode du sac de mots, avec CountVectorizer en backup,\n",
    "    - 1.2 coder TF-IDF, avec TfidfVectorizer en backup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f7d9b",
   "metadata": {},
   "source": [
    "### 1.1 Coder la méthode du sac de mots, avec CountVectorizer en backup:\n",
    "\n",
    "Faire la vectorisation en sac ce mots à la main (c.a.d, le calcul des vecteurs one-hot qui ont en fait plusieurs points \"hots\" (plusieurs 1 et pas un seul 1 dans le vecteur), avec une ou deux boucles).\n",
    "On vous guide plus bas.\n",
    "- recenser tous les types à mettre dans le vocabulaire à l'aide d'un `set` ou d'un `dict`. Compter combien vous avez de types différents (c'est la taille du vocabulaire, notée $V$).\n",
    "- Définir un `dict` qui enverra chaque type (une `str`) vers un indice (les `set` ne sont pas itérables de façon stable, d'ou le passage préférable vers cette table type->indice)\n",
    "- définir le `np.array` qui servira de résultat (notre X pour ensuite faire du Machine Learning)\n",
    "- passer en revue chaque texte d'intérêt, et:\n",
    "    - pour chaque token du texte, \n",
    "        - si il fait partie des types sélectionnés dans notre vocabulaire, noter son apparition (sinon, rien)\n",
    "- afficher le résultat (ou plutot des extraits), comparer à la vectorisation faite par sklearn, pour vérifier.\n",
    "\n",
    "Bonus: dans le vocabulaire, quand un mot n'apparait que 1 fois dans le corpus, ne pas le mettre dans le vocabulaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ce120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e27af5a",
   "metadata": {},
   "source": [
    "#### TODO: \n",
    "En utilisant la methode .update() associée aux objets de type `set` de python, définir le vocabulaire.\n",
    "\n",
    "Puis compter sa taille. On doit trouver 15030."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca234a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Vocabulaire = set()\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb9b04",
   "metadata": {},
   "source": [
    "#### TODO: \n",
    "Définir un dict qui pour chaque type, donne un indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25deba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2index ={}\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e98ff",
   "metadata": {},
   "source": [
    "#### TODO :\n",
    "Définir un array de zéros qu'on peut appeler X, qui donnera en ligne n, colonne v, la valeur booleene \"est ce que le type numéro v est présent dans le texte numéro n\".\n",
    "\n",
    "Ce calcul doit prendre un temps $O(N.V)+O($[nombre de tokens du corpus]$)$, mais pas $O(N.V^2)$ ou autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc95944",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum(axis=1)  #comptage du nombre de tokens differents (pour comparaison avec sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc8d55",
   "metadata": {},
   "source": [
    "#### Vérification par comparaison avec sklearn:\n",
    "\n",
    "Attention, l'ordre du Vocabulaire utilisé par sklearn ne sera pas forcément le meme que le votre, par défaut.\n",
    "Vous pouvez donc aussi lui fournir votre propre vocabulaire (2 choix possibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54105e8-6ed8-4706-abe7-c2873ef9fce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regexp = r\"\\b\\w+\\b\"\n",
    "skVocabulaire = None\n",
    "# skVocabulaire = list(Vocabulaire)\n",
    "skSacDeMot = CountVectorizer(vocabulary=skVocabulaire, binary=True, token_pattern=regexp)\n",
    "X_sacDeMots = skSacDeMot.fit_transform(texts)\n",
    "X_sacDeMots = X_sacDeMots.toarray()\n",
    "X_sacDeMots.sum(axis=1)  #comptage du nombre de tokens differents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94804ea7-7381-4aa5-9b2b-2691354812bc",
   "metadata": {},
   "source": [
    "## On visualise nos textes à l'aide d'une PCA \n",
    "(déjà vu en cours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d9b0f-a2c8-4bcc-8e49-a5e0bc06ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "maPCA = sklearn.decomposition.PCA(n_components=2)\n",
    "maPCA.fit(X_sacDeMots)\n",
    "X_transformee = maPCA.transform(X_sacDeMots)\n",
    "\n",
    "mycolorBar = cm.tab10\n",
    "plt.figure()\n",
    "plt.scatter(X_transformee[:,0], X_transformee[:,1], c=mycolorBar(y_numerique), marker= 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126e302",
   "metadata": {},
   "source": [
    "### Conclusion: c'est plutot prometteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18663ede",
   "metadata": {},
   "source": [
    "## 1.2 coder TF-IDF, avec sklearn en backup.\n",
    "\n",
    "- Définir le `np.array` qui servira de résultat (notre `X_TFIDF` pour ensuite faire du Machine Learning)\n",
    "- Calcul du IDF:\n",
    "    - Calculer le *document frequency* DF brut de chaque token, c.a.d. pour chaque token, le nombres de documents où le token apparaît au moins 1 fois, **sans diviser par le nombre de documents**. Astuce: utilisez le vecteur X (sac de mot, vecteur ne-hot) calculé juste avant.\n",
    "    - Le IDF du token est alors $np.log\\left( \\frac{Ndocs+1}{DF+1} \\right) + 1$  (on ajoute les +1 pour éviter de diviser par 0)\n",
    "- Pour chaque texte d'intérêt, calculer \n",
    "    - pour chacun de ses tokens, \n",
    "        - la *term frequency* TF, c.a.d. en fait le nombre d'occurces de ce token dans ce texte \n",
    "        - Le résultat (TF) permet de déduire  (pour chaque texte, et chaque type du vocabulaire), le score TF-IDF du token (notez que le score est 0 pour les types absents du texte).\n",
    "        - stockez le résultat dans votre `X_TFIDF`.\n",
    "- afficher le résultat (ou plutot des extraits), comparer à la vectorisation faite par sklearn, pour vérifier.\n",
    "\n",
    "Il est recommandé de ne pas faire de boucle qui implique tous les types du vocabulaire: mieux vaut boucler sur les tokens d'un texte (même si certains apparaissent plusieurs fois) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ef63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on calcule la Inverse Document Frequency:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## le code pour la Term frequency esy tres tres semblable au code du Sac de mot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ceee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TFIDF = (TF * IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TFIDF.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e1f96",
   "metadata": {},
   "source": [
    "## Vérification de notre TF-IDF avec TfidfVectorizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skTFIDF_vectorizer = TfidfVectorizer(token_pattern=regexp, min_df=0, norm=None, stop_words=None) \n",
    "## maybe more non-default choices will be needed to match our naive TF-IDF\n",
    "skX_TFIDF = skTFIDF_vectorizer.fit_transform(texts)\n",
    "skX_TFIDF = skX_TFIDF.toarray()\n",
    "skX_TFIDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60627f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "skX_TFIDF.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5a020",
   "metadata": {},
   "source": [
    "## On visualise nos textes à l'aide d'une PCA \n",
    "\n",
    "(on peut aussi comparer les PCA de notre version du code et de sklearn, pour voir si ça ressemble, si jamais les 2 outputs ne sont pas exactement identiques)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefa200",
   "metadata": {},
   "outputs": [],
   "source": [
    "maPCA = sklearn.decomposition.PCA(n_components=3)\n",
    "maPCA.fit(X_TFIDF)\n",
    "X_transformee = maPCA.transform(X_TFIDF)\n",
    "\n",
    "mycolorBar = cm.tab10\n",
    "plt.figure()\n",
    "plt.scatter(X_transformee[:,0], X_transformee[:,1], c=mycolorBar(y_numerique), marker= 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ce596",
   "metadata": {},
   "outputs": [],
   "source": [
    "maPCA = sklearn.decomposition.PCA(n_components=3)\n",
    "maPCA.fit(skX_TFIDF)\n",
    "X_transformee = maPCA.transform(skX_TFIDF)\n",
    "\n",
    "mycolorBar = cm.tab10\n",
    "plt.figure()\n",
    "plt.scatter(X_transformee[:,0], X_transformee[:,1], c=mycolorBar(y_numerique), marker= 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7e7e4",
   "metadata": {},
   "source": [
    "### Conclusion: ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de171d05",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "On utilise pour la suite les encodeurs de sklearn, qui sont sans erreurs et plus rapides que les votres)\n",
    "\n",
    "    - 2.1 utiliser un modèle Bayésien Naïf (sans PCA) comme classifieur  (pour le sac de mot, vs TF-IDF)\n",
    "    - 2.2 [optionnel] (si le temps permet) utiliser une PCA(D' variable)+k-NN(n variable) comme classifieur (pour le sac de mot, vs TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31359c0a",
   "metadata": {},
   "source": [
    "## 2.1 Bayesien Naïf\n",
    "\n",
    "- faire un train/test split de votre X,y (sac de mots) (utilisez `sklearn.model_selection.train_test_split`)\n",
    "- entrainer un classifieur `sklearn.naive_bayes.MultinomialNB`\n",
    "- calculer le score de cross-validation (jouer avec des hyper-paramètre éventuellement), pour les deux choix de pre-processing (sac de mot vs TF-IDF) (Utilisez `sklearn.model_selection.cross_validate`). Prenez par exemple cv=10 plis de cross-validation\n",
    "\n",
    "- Refaire tout ça en utilisant le TF-IDF à la place du sac de mots.\n",
    "- pour le meilleur des deux, calculer le test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1e75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fe4dc35",
   "metadata": {},
   "source": [
    "## Conclusion:  \n",
    "\n",
    "conclure ! Lequel est le meilleur ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049dc141",
   "metadata": {},
   "source": [
    "## Refaire la meme chose avec un plus gros dataset:\n",
    "\n",
    "Pour limiter l'overfitting en classification, on va prendre un plus gros dataset (20newsgroup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "## le train/test split est déja fourni:\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test  = fetch_20newsgroups(subset='test',  remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb289a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23365045",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.data[1], newsgroups_train.target_names[newsgroups_train.target[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainval = newsgroups_train.target\n",
    "y_test  = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aide: pensez au vectorizer ou TFIDF comme a une PCA : il y a un fit et un transform.\n",
    "## Sur quel ensemble applique t on le fit ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pour la cross-validation, a qui l'applique t'on ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72499f",
   "metadata": {},
   "source": [
    "## conclusion: lequel est le meilleur ?\n",
    "\n",
    "On peut maintenant calculer la test error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90965aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remarque: quel(s) ensemble de données peut on utiliser pour faire le fit ? Parmi Train, val, test ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216cd72",
   "metadata": {},
   "source": [
    "## Conclusion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fa878",
   "metadata": {},
   "source": [
    "## 2.2 Bonus: PCA + SVM\n",
    "\n",
    "Faire la classification avec le meilleur pre-processing, mais cette fois ci avec PCA+SVM. Optimiser `n_components` et `C`, par exemple (avec cross-validation évidemment, pas sur le test set !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a9d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f47254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
