{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import sklearn\n",
    "plt.ion()\n",
    "\n",
    "import sklearn.decomposition\n",
    "import sklearn.svm\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ici on vous donne une fonction très basique qui réalise un train/validation/split \n",
    "## il n'est PAS aléatoire\n",
    "## mais au moins, on voit bien ce qu'il se passe.\n",
    "def load_subSets(X, y, ratio_train=0.6, ratio_valid=0.2):\n",
    "    ratio_test = 1 - ratio_train - ratio_valid #\n",
    "    assert(ratio_test>0) ## on vérifie qu'il reste des exemples pour le test set\n",
    "    Ntot   = X.shape[0]\n",
    "    Ntrain = int(ratio_train*Ntot)\n",
    "    Nvalid = int(ratio_valid*Ntot)\n",
    "    Ntest  = Ntot - Ntrain - Nvalid\n",
    "    X_train = X[0: Ntrain].copy()\n",
    "    y_train = y[0: Ntrain].copy()\n",
    "    X_valid = X[Ntrain:Ntrain+Nvalid].copy() # X[-Ntest:] est equivalent\n",
    "    y_valid = y[Ntrain:Ntrain+Nvalid].copy() # X[-Ntest:] est equivalent\n",
    "    X_test  = X[-Ntest:].copy()\n",
    "    y_test  = y[-Ntest:].copy()\n",
    "    assert(y_test.shape[0]>1) ## on vérifie qu'il reste des exemples pour le test set (on re-vérifie à cause de l'arrondi...)\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 0 : choix du dataset\n",
    "\n",
    "Pour vous guider, vous pouvez consulter les pages 19-20 du poly, qui sont un énoncé synthétique, mais qui donne la vue d'ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: très bien pour commencer mais très petit, donc les expériences donneront des résultat plutôt peu intéressants dessus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data set 1 ##\n",
    "K=10\n",
    "import sklearn.datasets\n",
    "X,y = sklearn.datasets.load_digits(n_class=K, return_X_y=True)\n",
    "DimRepre = 8\n",
    "ratio_train = 0.6 # on peut reduire ce nombre lorsqu'on utilise le gros data set, mnist70.npz\n",
    "ratio_valid = 0.3 # on peut reduire ce nombre lorsqu'on utilise le gros data set, mnist70.npz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: mieux car plus grand, donc résultats plus intéressants. Attention, les expériences peuvent mettre du temps à tourner. \n",
    "\n",
    "Il peut être intéressant de commencer vos recherche sur une version plus petite du dataset (en gardant que 10% des data par exemple), et quand votre code marche bien et que vous avez resséré la plage de valeurs des hyper-paramètre qui est vraisemblablement la bonne, faire une recherche plus précise sur l'ensemble du dataset.\n",
    "\n",
    "Astuce: en particulier, n'utilisez pas un grand nombre de composantes PCA sur ce dataset (en tout cas ne faites pas PCA(grand n_comp) + featureMap_polynomiale(degré2) + SVM là dessus.\n",
    "\n",
    "À aller chercher sur : https://gitlab.inria.fr/flandes/data-for-teaching\n",
    "\n",
    "Dans le shell linux, faites: \n",
    "`wget https://gitlab.inria.fr/flandes/data-for-teaching/-/raw/master/mnist70.npz?inline=false`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set 2 ##\n",
    "LoadObject = np.load(\"/home/flandes/data/mnist70.npz\") ## adresse à adapter à votre cas bien sur !\n",
    "DimRepre = 28\n",
    "X = LoadObject['X']\n",
    "y = LoadObject['y']\n",
    "del LoadObject\n",
    "ratio_train = 0.1  ## !! ici on ne garde que 10% pour train, pour que ça tourne vite, lorsqu'on développe le code !\n",
    "ratio_valid = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3: même remarques que pour le 2\n",
    "\n",
    "À aller chercher sur : https://gitlab.inria.fr/flandes/data-for-teaching\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## data set 3 ##\n",
    "# LoadObject = np.load(\"fashion-mnist-reshaped.npz\")\n",
    "# DimRepre = 28\n",
    "# X = LoadObject['train_images']\n",
    "# y = LoadObject['train_labels']\n",
    "# Xtest = LoadObject['test_images']\n",
    "# ytest = LoadObject['test_labels']\n",
    "# X     = np.array(X    , dtype=float) ## on change le type, car c'est unsigned-int (uint) au depart\n",
    "# Xtest = np.array(Xtest, dtype=float)\n",
    "# del LoadObject\n",
    "# ratio_train = 0.1\n",
    "# ratio_valid = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 --  PCA: prise en main\n",
    "Ici, on souhaite faire une première PCA sur les données de MNIST (où n’importe quelles données visuelles).\n",
    "1. Consultez la partie 1 du TP (fichier .ipynb) [vous y êtes !]\n",
    "2. Calculer la transformée en PCA des données en conservant 95% de la variance des données. Doit-on\n",
    "calculer la PCA seulement sur les données d’entraînement, ou bien sur l’ensemble (train+validation) ?\n",
    "Pourquoi ? (Quel est le risque, dans le mauvais cas ?)\n",
    "3. Extraire le nombre de composantes effectivement retenues par la décomposition.\n",
    "4. [on vous donne le code] observer le tracé du ratio de variance expliquée en fonction du nombre de composantes. \n",
    "5. À partir de la version transformée des données, calculer aussi leur version décompressée (par transformée\n",
    "inverse). Afficher quelques exemples de avant/après (compression+décompression). On peut jouer avec\n",
    "le taux de variance expliquée et voir l’effet sur les images. (il est recommandé d’écrire une fonction).\n",
    "6. (optionnel) Calculer l’erreur de reconstruction moyenne sur toutes les images (c’est plus facile que pour une seule\n",
    "image). Le faire aussi pour l’ensemble de valdiation.\n",
    "7. (optionnel) Calculer l’erreur de reconstruciton pour une seule image. (inclure ceci dans la fonction de comparaison\n",
    "avant/après).\n",
    "\n",
    "Remarque: la variance expliquée est le conjugué de l'erreur quadratique moyenne de reconstruction. \n",
    "En effet, la variance (expliquée) des données correspond à la dispersion (restante) entre les valeurs des pixels (moyennée sur l’ensemble du dataset).\n",
    "L’erreur de reconstruction mesure le taux d'erreur (quadratique moyenne), entre l’image de départ et l ímage reconstruite, pixel par pixel (et moyennée ensuite sur toutes les images). \n",
    "Ce sont deux notions différentes mais qui sont en correspondance: une plus grande variance expliquée correspond toujours à une plus petite erreur de reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "n=41 # image numero 42\n",
    "plt.imshow(X[n].reshape(DimRepre, DimRepre) , cm.gray)\n",
    "plt.title(\"ceci est censé ressembler à un \"+str(y[n])+ \" avant la binarisation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# on met X (les intensités de niveaux de gris) entre 0 et 1\n",
    "# (pour certains datasets, c'est nécessaire, pour d'autres c'est déjà fait)\n",
    "X /= ?? # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train = ??\n",
    "ratio ??\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_subSets(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ici, allez voir la doc de `sklearn.decomposition.PCA`\n",
    "\n",
    "en particulier, il y est question d'un argument d'entrée `n_components` et d'un attribut  `explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sklearn.decomposition.PCA \n",
    "## c'est plus joli en ligne mais on a aussi la doc offline ! \n",
    "## pratique quand on ne se souvient plus du nom des arguments par exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 1er essai de PCA avec \"ratio de variance expliquee\" fixe a 0.95\n",
    "\n",
    "# if premiere_partie_qq_essais_a_la_main:\n",
    "varianceExplained = 0.95\n",
    "preProc = sklearn.decomposition.PCA( ?? ) ## TODO: fixer les hyper-parametres de cette transformation\n",
    "preProc.fit( ?? ) ## TODO: à éditer pour que ça marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: combien y a t il de composantes qui ont ete retenues ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tracé de la variance expliquée au total pour tous les choix de n_component réalisables \n",
    "## (pour des variances expliquees entre 0 et 0.95)\n",
    "CumulativeExplainedVariance = np.cumsum(preProc.explained_variance_ratio_)\n",
    "plt.plot(CumulativeExplainedVariance, marker='+', label=\"ensemble de ?? TODO ??\")\n",
    "plt.xlabel(\"?? TODO ??\")\n",
    "plt.ylabel(\" ?? TODO ??\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explication complémentaire:\n",
    "\n",
    "Petite explication sur l'utilisation du parametre `n_components` dans la classe PCA de sklearn :\n",
    "\n",
    "Python est tres flexible, donc:\n",
    "- si la valeur passée pour `n_components` est de type `int`, alors sklearn interprète cette valeur comme le nombre de composantes à garder dans la PCA (c'est le $D'$ de mon cours)\n",
    "- si la valeur passée pour `n_components` est de type `float`  (et on espère, entre 0 et 1), sklearn l'interprète comme un ratio de variance expliquée désirée.\n",
    "Dans ce 2ème cas, l'idée est la suivante: la variance des données est aussi égale à la somme des valeurs propres ($\\lambda_1+\\lambda_2+\\ldots +\\lambda_D$).\n",
    "Si on garde par exemple que 2 composantes (les 2 premières), on aura une variance des données apres projection égale a $\\lambda_1+\\lambda_2$.\n",
    "Le ratio de variance \"expliquée\"  (c.a.d. préservée par la PCA) sera de $$\\frac{\\lambda_1+\\lambda_2}{\\lambda_1+\\lambda_2+\\ldots +\\lambda_D}$$\n",
    "Ainsi,  si la valeur passée pour `n_components` est de type `float`, sklearn cherche des $\\lambda$ jusqu'à ce que leur somme fasse 95% de la variance totale des données. Ainsi, il choisit le $D'$ à votre place... mais bon, vous avez choisi 95%, ce qui est aussi arbitraire (mais moins arbitraire que dire \"gardons 10 composantes\" ou bien \"gardons 10% des composantes\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si vous êtes allés assez vite, vous pouvez faire tout ou partie de cette liste: \n",
    "\n",
    "(optionnel) \n",
    "- TODO: pour une image apres \"compression\" par PCA, calculer l'image décompressee correspondante.\n",
    "- TODO: calculer ensuite l'erreur de reconstruction (erreur quadratique moyenne sur les pixels) pour une image *[éventuellement, faites ceci dans une fonction pour pouvoir rapidement comparer visuellement n'importe quel exemple d'entrainement avant et apres compression/decompression.]*\n",
    "- TODO: calculer la version decompressée de tout le train-set.\n",
    "- TODO : calculer l'erreur quadratique moyenne sur tout le train-set (indice: ca se fait en 1 seule ligne)\n",
    "(vous pouvez traiter directement tout le train set, c'est en fait plus simple)\n",
    "\n",
    "### Sinon, passez à la suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 -- Optimisation du nombre de composantes\n",
    "\n",
    "Maintenant qu'on a compris un peu comment fonctionne la PCA, on va l'utiliser comme pré-processing:\n",
    "\n",
    "1. Consultez la partie 2 du TP (fichier .ipynb) [vous y êtes !]\n",
    "2. En faisant varier explicitement le nombre de composantes (nComp) à retenir dans la PCA, observez\n",
    "comment les performances dépendent de ce nombre de composantes.\n",
    "3. Sur le graphe, indiquer le point de meilleur score (le code vous est déjà donné, mais esayez de la faire par\n",
    "vous même/de bien comprendre le code).\n",
    "\n",
    "## Plus en détail:\n",
    "\n",
    "- Commencez par utiliser la PCA comme pré processing (prenez un nombre de composantes  `n_components`  arbitraire, peu importe combien) c.a.d, transformez les données, puis, sur ces données \"compressées\" (après PCA), utilisez un classifieur:\n",
    "    + consultez la doc de `sklearn.svm.SVC`\n",
    "    + créer un SVM avec noyau (`kernel`) polynomial de degré 2\n",
    "    + faites le fit de ce modèle, sur les données d'entrainement (transformées)\n",
    "    + évaluez le modèle (calculez le score) sur les données d'entrainement (transformées)\n",
    "    + évaluez le modèle (calculez le score) sur les données de validation (transformées aussi: essayez sans, vous verrez :P)\n",
    "- une fois que ça marche pour un choix de `n_components` arbitraire, faites une boucle pour comparer plein de valeurs de `n_components`\n",
    "\n",
    "### Conseil: \n",
    "commencez avec le dataset 1 !! Votre code tournera vite, c'est pratique pour développer :)\n",
    "Et pour le dataset 2, n'allez pas jusqu'à D=784 composantes. Arretez vous à 64 par exemple, en allant de 4 en 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train = ??\n",
    "ratio_valid = ??\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_subSets(X, y, ratio_train, ratio_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProc = sklearn.decomposition.PCA( ?? ) ## TODO\n",
    "preProc.fit( ?? )  ## TODO\n",
    "## TODO: transformer les donnees selon cette transformation apprise (comme à l'étape d'avant)\n",
    "\n",
    "X_train_Transformed = ?? # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: faire l'apprentissage supervisé des classes, à l'aide d'un SVM polynomial de degre 2 (et avec C=1, coef0 = 1)\n",
    "# clf = ## TODO\n",
    "clf.fit(X_train_Transformed, y_train)  ## utilisez les données transformées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: calculer les scores a l'aide de clf.score() :\n",
    "# trainscore = # TODO\n",
    "# validscore = # TODO\n",
    "# print(\"nombre Composants\", nC , \"   training score:\",trainscore, \". valid score:\", validscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maintenant, mettons tout ça dans une boucle sur `n_components`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_training_score = []\n",
    "linear_valid_score = []\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_subSets(X,y)\n",
    "## TODO: pour differentes valeurs de n_components, realiser une PCA\n",
    "## puis une classification\n",
    "nComp_range =\n",
    "for nC in nComp_range:\n",
    "    ## TODO: la meme chose qu'au dessus, \n",
    "    ## + enregistrer les resultats (train, valid scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot des scores en fonction de l'hyper-param. nc\n",
    "## c'est cadeau (rien à faire ici)\n",
    "plt.figure()\n",
    "plt.plot(nComp_range, linear_training_score, label= \"train score\")\n",
    "plt.plot(nComp_range, linear_valid_score   , label= \"valid score\")\n",
    "plt.xlabel(\"nombre comp\")\n",
    "plt.ylabel(\"scores\")\n",
    "plt.legend()\n",
    "plt.ylim([0.5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération de l'indice du meilleur choix de l'hyper-paramètre `n_components`\n",
    "\n",
    "Ici on fait l'affichage du meilleur choix d'hyper-param (pour l'ensemble de validation évidemment).\n",
    "Ici on vous montre un choix primaire, sans tenir compte de l'overfitting, etc: il peut et doit être affiné en observant les courbes (choix \"à la main\", à l'aide de tracés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestIndex = np.argmax(linear_valid_score) ## on trouve l'indice tel que la performance est la meilleure\n",
    "bestNC = nComp_range[bestIndex] ## on récupère la valeur de l'hyper-param qui correspond\n",
    "plt.plot(bestNC, linear_valid_score[bestIndex], marker='X', color='green') ## on affiche le point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 bis: Cross-Validation\n",
    "\n",
    "On a fait tout ceci pour un partage donné du train et validation set.\n",
    "\n",
    "Pour être robuste vis-à-vis de l'aléatoire associé au tirage des données, il vaut mieux utiliser la cross-validation. Ceci est vrai en particulier pour les jeux de données avec peu d'exemple (N petit)\n",
    "\n",
    "- faites un partage (tran+validation)/test des données, de façon déterministe (pas aléatoire), en vous inspirant de la fonction `load_subSets` donnée plus haut.\n",
    "- faites vous meme une fonction de Cross-Validation:\n",
    "    + faire un tableaux d'indices de 0 à N-1, mais aléatoire (mélangez les indices. Ça se dit *shuffle* en anglais) \n",
    "    + faites une méthode qui permet de piocher de façon déterminsite une tranche parmi 5 (par exemple), en choisissant la tranche\n",
    "    + testez votre fonction (vérifier que quand on demande la tranche 0, puis la 1, ...  la 4, ça donne bien tout les exemples du datset, chacun étant vu une seule fois en tant que validation\n",
    "- Utilisez votre méthode CrossValidation:\n",
    "- pour une valeur de l'hyper-paramètre, faire 5 tirages train/validation, et mesurer les 5 scores (train+val, donc 10); enregistrer tout ces résultats\n",
    "- faire à nouveau la boucle sur les valeurs de l'hyper-paramètre; enregistrer tout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 -- Optimisation double: nComp et C\n",
    "1. Consultez la partie 3 du TP (fichier .ipynb) [vous y êtes!]\n",
    "2. Vous devez chercher à optimiser à la fois `n_components` et le paramètre `C` asocié à la réglarisation du SVM. Une\n",
    "grosse part du travail à déjà été fait. Prenez garde à appliquer la transformation PCA au bon moment,\n",
    "de la bonne façon.\n",
    "3. Le tracé de la figure (score en fonction des deux hyper-paramètres) est déjà fourni. Lisez le attentivement\n",
    "(vous pouvez aussi l’améliorer, par exemple en faisant un tracé \"en 3D\").\n",
    "4. En cherchant dans la doc de np.argmax et de np.unravel, obtenez les coordonnés du point qui réalise le\n",
    "maximum de score, et affichez le.\n",
    "5. (optionnel) Refaire tout cela mais en utilisant par exemple le modèle des k plus proches voisins (k-NN, cf. la doc. de la classe `sklearn.neighbors.KNeighborsClassifier`, en faisant varier le nombre de voisins au lieu de C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComp_range = np.arange(1,64,4)\n",
    "hyperParam_range = [10**k  for k in range(-4,4,1)]\n",
    "kernel = 'poly'\n",
    "degree = 2\n",
    "\n",
    "## si vous suivez cette convention d'enregistrement, le code (donné ci dessous) pour afficher le résultat\n",
    "## fonctionnera directement. Si vous choisissez une autre convention, il faudra adapter le code..\n",
    "array_training_score = np.zeros( (len(nComp_range), len(hyperParam_range)) )\n",
    "array_validati_score = np.zeros( (len(nComp_range), len(hyperParam_range)) )\n",
    "\n",
    "ratio_train = 0.4\n",
    "ratio_valid = 0.4\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_subSets(X, y, ratio_train, ratio_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, nC in enumerate(nComp_range):\n",
    "    X_train_Transformed = ?? # TODO\n",
    "    ## TODO : completer ici\n",
    "    for j, mu in enumerate(hyperParam_range):\n",
    "        ## TODO: completer ici\n",
    "        array_training_score[i,j] = trainscore\n",
    "        array_validati_score[i,j] = validscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On vous donne le code qui affiche la figure (si vous avez suivi ma façon d'enregistrer les résultats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trace de la figure ###\n",
    "plt.figure()\n",
    "levels=np.array([0.5,0.6,0.7,0.8,0.9,0.95,1])\n",
    "# levels=np.arange(0.8,1.0,0.01) ## pour un trace plus fin\n",
    "plt.contourf(array_validati_score, levels=levels)\n",
    "architecture = \"SVM\" ## on pourrait essayer d'autres classifieurs, comme k-NN par exemple.\n",
    "if architecture=='SVM':\n",
    "    nameofHyperParam = 'C (SVM)'\n",
    "else:\n",
    "    nameofHyperParam = 'k (k-NN)'\n",
    "plt.xlabel(\"hyper-param \"+nameofHyperParam)\n",
    "xtick = np.arange(len(hyperParam_range))\n",
    "plt.xticks(xtick, hyperParam_range)\n",
    "plt.ylabel(\"num. of Comp (PCA)\")\n",
    "ytick = np.arange(len(nComp_range))\n",
    "plt.yticks(ytick, nComp_range)\n",
    "plt.colorbar()\n",
    "# indice_max = ## TODO chercher par vous meme (ou stackoverflow) comment trouver le argmax d'un tableau a deux entrées\n",
    "# plt.scatter(indice_max[1],indice_max[0] , marker='X', color='red')\n",
    "plt.savefig(\"hyper-param-tuning_arci=\"+architecture+\".png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4: avec plus de sklearn\n",
    "\n",
    "Il était utile de faire tout ça à la main une fois, pour bien comprendre la validation, le pré-processing, mais :\n",
    "- en fait, on peut automatiser tout ça, par exemple avec `sklearn.model_selection.cross_validate`  et `Pipeline`\n",
    "- lisez la doc et faites tout ça en qq lignes !\n",
    "- enjoy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
